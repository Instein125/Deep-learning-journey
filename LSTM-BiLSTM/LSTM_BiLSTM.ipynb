{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementing LSTM using keras LSTM layer"
      ],
      "metadata": {
        "id": "CBIkKfjd-5cN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl7uLchG-wNn",
        "outputId": "33b44e7a-2bdd-48a7-a362-94bb17468fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 10, 3)             72        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72\n",
            "Trainable params: 72\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# import LSTM layer\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# to create lstm we have to provide input shape to the lstm layer\n",
        "# generally the input is a vector of dimension n, but we also need to provide\n",
        "# the number of time steps as well as the number of training set we have\n",
        "\n",
        "# create a sequential model first\n",
        "model = Sequential()\n",
        "\n",
        "# input_shape=(number_of_time_steps, size_of_input_vector)\n",
        "# define input_shape with\n",
        "# number of time steps: 10\n",
        "# number of features (size of feature vector): 5\n",
        "# input shape is just a tuple with two elements (2-tuple)\n",
        "input_shape = (10, 2)\n",
        "\n",
        "# let's define the shape of output vector (dimensionality of output space)\n",
        "output_units = 3 #This indicates we will have 3 LSTM cell\n",
        "\n",
        "# add a lstm layer to this model\n",
        "model.add(LSTM(output_units, input_shape=input_shape, return_sequences=True))\n",
        "\n",
        "# providing return_sequences True makes the network to return output at every timestep\n",
        "\n",
        "# let's print the sumary of our model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Bidirectional LSTM using keras"
      ],
      "metadata": {
        "id": "-mYd8c1U_i1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "nomD4nt2_fvE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(20000, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBBRyad2_6LJ",
        "outputId": "61890621-b024-4636-9927-fb64199b9ae9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, None, 128)        98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,757,761\n",
            "Trainable params: 2,757,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading IMDB dataset"
      ],
      "metadata": {
        "id": "95bfZP8PBDG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=20000\n",
        ")\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=50)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuj1xbdKA23i",
        "outputId": "62fc4b0e-b8d7-45c3-cfcb-6a142deb7aed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluating the model"
      ],
      "metadata": {
        "id": "3wW7hScmBN5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LraOT4TDBJ_o",
        "outputId": "91b7d0ac-cd8e-47d4-f97d-54ae5fbd1750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 206s 250ms/step - loss: 0.4526 - accuracy: 0.7822 - val_loss: 0.3864 - val_accuracy: 0.8236\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 193s 247ms/step - loss: 0.2606 - accuracy: 0.8926 - val_loss: 0.4091 - val_accuracy: 0.8155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7mVK0L9BSMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}