{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFQnHsHCpS1q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = [ \"The quick brown fox jumps over the lazy dog\",\n",
        "\"Tomorrow is going to be a sunny day\",\n",
        "'She opened the door and found a surprise waiting for her',\n",
        "'I enjoy reading books in my free time',\n",
        "'The concert was amazing, and the crowd cheered loudly',\n",
        "'He took a deep breath and stepped onto the stage',\n",
        "'The smell of freshly baked cookies filled the kitchen',\n",
        "'The little girl giggled as the puppy licked her face',\n",
        "'After a long day at work, he decided to go for a walk in the park',\n",
        "'The old house creaked in the wind, creating an eerie atmosphere',]"
      ],
      "metadata": {
        "id": "gQlhNdHDquA9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "1WQ67tprrZGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<nothing>')\n",
        "\n",
        "# oov_token specifies the out-of-vocabulary (OOV) token.\n",
        "# The OOV token is used to represent words or tokens that are not present in the vocabulary learned by the Tokenizer.\n",
        "# In this case, the OOV token is set to <nothing>, meaning that any unseen words will be represented by this token during tokenization."
      ],
      "metadata": {
        "id": "H07C157OrFBn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(text_dataset)"
      ],
      "metadata": {
        "id": "0uKGn5kgrz3-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIl6Pu8fr4MG",
        "outputId": "9c070ab8-a44e-4df4-a551-a590cc2fb390"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<nothing>': 1,\n",
              " 'the': 2,\n",
              " 'a': 3,\n",
              " 'and': 4,\n",
              " 'in': 5,\n",
              " 'to': 6,\n",
              " 'day': 7,\n",
              " 'for': 8,\n",
              " 'her': 9,\n",
              " 'he': 10,\n",
              " 'quick': 11,\n",
              " 'brown': 12,\n",
              " 'fox': 13,\n",
              " 'jumps': 14,\n",
              " 'over': 15,\n",
              " 'lazy': 16,\n",
              " 'dog': 17,\n",
              " 'tomorrow': 18,\n",
              " 'is': 19,\n",
              " 'going': 20,\n",
              " 'be': 21,\n",
              " 'sunny': 22,\n",
              " 'she': 23,\n",
              " 'opened': 24,\n",
              " 'door': 25,\n",
              " 'found': 26,\n",
              " 'surprise': 27,\n",
              " 'waiting': 28,\n",
              " 'i': 29,\n",
              " 'enjoy': 30,\n",
              " 'reading': 31,\n",
              " 'books': 32,\n",
              " 'my': 33,\n",
              " 'free': 34,\n",
              " 'time': 35,\n",
              " 'concert': 36,\n",
              " 'was': 37,\n",
              " 'amazing': 38,\n",
              " 'crowd': 39,\n",
              " 'cheered': 40,\n",
              " 'loudly': 41,\n",
              " 'took': 42,\n",
              " 'deep': 43,\n",
              " 'breath': 44,\n",
              " 'stepped': 45,\n",
              " 'onto': 46,\n",
              " 'stage': 47,\n",
              " 'smell': 48,\n",
              " 'of': 49,\n",
              " 'freshly': 50,\n",
              " 'baked': 51,\n",
              " 'cookies': 52,\n",
              " 'filled': 53,\n",
              " 'kitchen': 54,\n",
              " 'little': 55,\n",
              " 'girl': 56,\n",
              " 'giggled': 57,\n",
              " 'as': 58,\n",
              " 'puppy': 59,\n",
              " 'licked': 60,\n",
              " 'face': 61,\n",
              " 'after': 62,\n",
              " 'long': 63,\n",
              " 'at': 64,\n",
              " 'work': 65,\n",
              " 'decided': 66,\n",
              " 'go': 67,\n",
              " 'walk': 68,\n",
              " 'park': 69,\n",
              " 'old': 70,\n",
              " 'house': 71,\n",
              " 'creaked': 72,\n",
              " 'wind': 73,\n",
              " 'creating': 74,\n",
              " 'an': 75,\n",
              " 'eerie': 76,\n",
              " 'atmosphere': 77}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfuX5ToYr6ga",
        "outputId": "ea6463cd-41fa-4389-bd33-574885a2d3aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('the', 13),\n",
              "             ('quick', 1),\n",
              "             ('brown', 1),\n",
              "             ('fox', 1),\n",
              "             ('jumps', 1),\n",
              "             ('over', 1),\n",
              "             ('lazy', 1),\n",
              "             ('dog', 1),\n",
              "             ('tomorrow', 1),\n",
              "             ('is', 1),\n",
              "             ('going', 1),\n",
              "             ('to', 2),\n",
              "             ('be', 1),\n",
              "             ('a', 5),\n",
              "             ('sunny', 1),\n",
              "             ('day', 2),\n",
              "             ('she', 1),\n",
              "             ('opened', 1),\n",
              "             ('door', 1),\n",
              "             ('and', 3),\n",
              "             ('found', 1),\n",
              "             ('surprise', 1),\n",
              "             ('waiting', 1),\n",
              "             ('for', 2),\n",
              "             ('her', 2),\n",
              "             ('i', 1),\n",
              "             ('enjoy', 1),\n",
              "             ('reading', 1),\n",
              "             ('books', 1),\n",
              "             ('in', 3),\n",
              "             ('my', 1),\n",
              "             ('free', 1),\n",
              "             ('time', 1),\n",
              "             ('concert', 1),\n",
              "             ('was', 1),\n",
              "             ('amazing', 1),\n",
              "             ('crowd', 1),\n",
              "             ('cheered', 1),\n",
              "             ('loudly', 1),\n",
              "             ('he', 2),\n",
              "             ('took', 1),\n",
              "             ('deep', 1),\n",
              "             ('breath', 1),\n",
              "             ('stepped', 1),\n",
              "             ('onto', 1),\n",
              "             ('stage', 1),\n",
              "             ('smell', 1),\n",
              "             ('of', 1),\n",
              "             ('freshly', 1),\n",
              "             ('baked', 1),\n",
              "             ('cookies', 1),\n",
              "             ('filled', 1),\n",
              "             ('kitchen', 1),\n",
              "             ('little', 1),\n",
              "             ('girl', 1),\n",
              "             ('giggled', 1),\n",
              "             ('as', 1),\n",
              "             ('puppy', 1),\n",
              "             ('licked', 1),\n",
              "             ('face', 1),\n",
              "             ('after', 1),\n",
              "             ('long', 1),\n",
              "             ('at', 1),\n",
              "             ('work', 1),\n",
              "             ('decided', 1),\n",
              "             ('go', 1),\n",
              "             ('walk', 1),\n",
              "             ('park', 1),\n",
              "             ('old', 1),\n",
              "             ('house', 1),\n",
              "             ('creaked', 1),\n",
              "             ('wind', 1),\n",
              "             ('creating', 1),\n",
              "             ('an', 1),\n",
              "             ('eerie', 1),\n",
              "             ('atmosphere', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FttWIzD0sAfQ",
        "outputId": "45d780cf-c1f4-4cd6-fb4c-5d9b17f51e6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text to sequences"
      ],
      "metadata": {
        "id": "DyAr_IKGsJ6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(text_dataset)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgwPXDvtsFPl",
        "outputId": "43dfc499-7e2a-4cb2-9c3a-82b685dcbee5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 11, 12, 13, 14, 15, 2, 16, 17],\n",
              " [18, 19, 20, 6, 21, 3, 22, 7],\n",
              " [23, 24, 2, 25, 4, 26, 3, 27, 28, 8, 9],\n",
              " [29, 30, 31, 32, 5, 33, 34, 35],\n",
              " [2, 36, 37, 38, 4, 2, 39, 40, 41],\n",
              " [10, 42, 3, 43, 44, 4, 45, 46, 2, 47],\n",
              " [2, 48, 49, 50, 51, 52, 53, 2, 54],\n",
              " [2, 55, 56, 57, 58, 2, 59, 60, 9, 61],\n",
              " [62, 3, 63, 7, 64, 65, 10, 66, 6, 67, 8, 3, 68, 5, 2, 69],\n",
              " [2, 70, 71, 72, 5, 2, 73, 74, 75, 76, 77]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding"
      ],
      "metadata": {
        "id": "vXjuCmZtsTdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "sequences = pad_sequences(sequences, padding ='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOVGUVCLsR2e",
        "outputId": "eaf1b410-9a72-4a7f-f6be-6bfb6aa4bb68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2, 11, 12, 13, 14, 15,  2, 16, 17,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [18, 19, 20,  6, 21,  3, 22,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [23, 24,  2, 25,  4, 26,  3, 27, 28,  8,  9,  0,  0,  0,  0,  0],\n",
              "       [29, 30, 31, 32,  5, 33, 34, 35,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2, 36, 37, 38,  4,  2, 39, 40, 41,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [10, 42,  3, 43, 44,  4, 45, 46,  2, 47,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2, 48, 49, 50, 51, 52, 53,  2, 54,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2, 55, 56, 57, 58,  2, 59, 60,  9, 61,  0,  0,  0,  0,  0,  0],\n",
              "       [62,  3, 63,  7, 64, 65, 10, 66,  6, 67,  8,  3, 68,  5,  2, 69],\n",
              "       [ 2, 70, 71, 72,  5,  2, 73, 74, 75, 76, 77,  0,  0,  0,  0,  0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3l8X7GuEsdjP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}